{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9c3ba-5654-45b4-aa5e-0c5749f9c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import lime.lime_tabular, shap, dice_ml\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#todo: General step  \n",
    "# import test and training data and test data\n",
    "# preprocess it\n",
    "# \n",
    "#\n",
    "#\n",
    "#   \n",
    "\n",
    "def shap_xai(model_evaluation_list):\n",
    "    model, X_test, y_test, model_name = model_evaluation_list\n",
    "    explainer = None\n",
    "    if model_name == \"Linear Regression\":\n",
    "        explainer = shap.LinearExplainer(model, X_test)\n",
    "    if model_name == \"Random Forest\":\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "    if model_name == \"Gradient Boost\":\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        \n",
    "    # Compute contriution of each feature to the models prediction\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    feature_names = [f'Feature_{i}' for i in range(X_test.shape[1])]\n",
    "    \n",
    "    # X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "    X_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "    # Visualize Feature importance\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)\n",
    "    \n",
    "def lime_xai(model_evaluation_list):\n",
    "    model, X_test, y_test, model_name = model_evaluation_list\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_test, mode=\"classification\"\n",
    "    )\n",
    "    explanation = explainer.explain_instance(X_test[0], model.predict_proba)\n",
    "    explanation.show_in_notebook()\n",
    "\n",
    "def dice_xai(model_evaluation_list):\n",
    "    model, X_test, y_test, model_name = model_evaluation_list   \n",
    "    return \"Thomas Kitaba\"\n",
    "\n",
    "\n",
    "\n",
    "def model_evaluation(model_evaluation_list):\n",
    "    model, X_test, y_test, model_name = model_evaluation_list\n",
    "    result = {}\n",
    "    \n",
    "    # Start testing and Evaluation\n",
    "    model_predicted = model.predict(X_test)\n",
    "    model_predicted_prob = model.predict_proba(X_test)[:, 1]  # Get predicted probabilities for ROC-AUC\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, model_predicted)\n",
    "    F1_score = f1_score(y_test, model_predicted)\n",
    "    Precision_score = precision_score(y_test, model_predicted)\n",
    "    Recall_score = recall_score(y_test, model_predicted)\n",
    "    \n",
    "    Roc_auc_score = roc_auc_score(y_test, model_predicted_prob)\n",
    "    \n",
    "    result[\"model_name\"] = f\"{model_name}\"\n",
    "    result[\"accuracy\"] = accuracy\n",
    "    result[\"f1_score\"] = F1_score\n",
    "    result[\"precision_score\"] = Precision_score\n",
    "    result[\"recall_score\"] = Recall_score\n",
    "    result[\"roc_auc_score\"] = Roc_auc_score\n",
    "    return result\n",
    "    \n",
    "    \n",
    "    return(\"Thomas Kitaba\")\n",
    "    \n",
    "\n",
    "def linear_regression(X_train, y_train):\n",
    "    \"\"\"\n",
    "    train and test dataset using linear regression learning model\n",
    "    X_train: training data\n",
    "    y_train: target data\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    # Chose Model\n",
    "    lgr_model = LogisticRegression(max_iter=500)\n",
    "\n",
    "    # Start Training\n",
    "    lgr_model.fit(X_train, y_train)\n",
    "    \n",
    "        \n",
    "    return lgr_model\n",
    "\n",
    "\n",
    "def random_forest(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train and test dataset using Random Forest learning model\n",
    "    X_train: training data\n",
    "    y_train: target data\n",
    "    \"\"\"\n",
    "    # Chose Model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42) #n_estiators = number of trees, randome_state = for reprocuction\n",
    "    \n",
    "    # Start Training\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    return rf_model  # This is the trained model\n",
    "    \n",
    "def gradient_boost(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train and test dataset using Gradient Boosting learning model\n",
    "    X_train: training data\n",
    "    y_train: target data\n",
    "    \"\"\"\n",
    "    # Chose Model\n",
    "    gb_model = XGBClassifier(eval_metric=\"logloss\") #\n",
    "    # Start Training\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    # Start Testing\n",
    "    return gb_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Thomas kitaba\")\n",
    "   \n",
    "    # np.random.seed(42)\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
    "                           n_redundant=5, n_clusters_per_class=2, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "     # import csv files for traingn and testing\n",
    "    # training_dataframe = pd.read_csv('cs-training.csv', na_values='NA')\n",
    "    # test_dataframe = pd.read_csv('cs-test.csv', na_values='NA')\n",
    "    \n",
    "    # X_train = training_dataframe.iloc[:, :-1] # This are the training Features\n",
    "    # y_train = training_dataframe.iloc[:, -1]  # This are the test lables\n",
    "    \n",
    "    # X_test = test_dataframe.iloc[:, :-1] # This are the test Features\n",
    "    # y_test = test_dataframe.iloc[:, -1] # This are the test lables\n",
    "    #initialize an empty list to hold the three trained models\n",
    "    model_evaluation_list = []\n",
    "    \n",
    "    # Train model using Linear Regression\n",
    "    model_evaluation_list.append([linear_regression(X_train, y_train), X_test, y_test, \"Linear Regression\"]) # recive only model name\n",
    "\n",
    "    # Train Model Using Randome Forest\n",
    "    model_evaluation_list.append([random_forest(X_train, y_train), X_test, y_test, \"Random Forest\"]) # recive only model name\n",
    "    \n",
    "    # Train model using Gradient boost\n",
    "    model_evaluation_list.append([gradient_boost(X_train, y_train), X_test, y_test, \"Gradient Boost\"])\n",
    "    \n",
    "    all_models = []\n",
    "    for model in model_evaluation_list:\n",
    "        model_evaluation_results = model_evaluation(model)\n",
    "        all_models.append(model_evaluation_results)\n",
    "        print(model_evaluation_results)\n",
    "        # shap_xai(model)\n",
    "        lime_xai(model)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
